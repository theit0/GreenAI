{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerias"
      ],
      "metadata": {
        "id": "gn9tJUKLKYX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7q0t2QhhKWXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descomprimir dataset de kaggle"
      ],
      "metadata": {
        "id": "UJCoHjvF9wOo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKCqPql1w8pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123bdbd3-ffba-461c-8585-610ebf3e0bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GreenAI'...\n",
            "remote: Enumerating objects: 2558, done.\u001b[K\n",
            "remote: Total 2558 (delta 0), reused 0 (delta 0), pack-reused 2558 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2558/2558), 123.02 MiB | 15.71 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Clonar repositorio\n",
        "!git clone https://github.com/theit0/GreenAI.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/GreenAI/data/archive.zip'"
      ],
      "metadata": {
        "id": "NQ9alxA-MLoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "extract_path = '/content/descomprimido'\n",
        "\n",
        "# Crear carpeta si no existe\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Descomprimir\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Descomprimido en:\", extract_path)"
      ],
      "metadata": {
        "id": "cYLfHMdgoBz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9f7653-407e-493c-e3d0-8926114bd65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descomprimido en: /content/descomprimido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración del dispositivo"
      ],
      "metadata": {
        "id": "T_GtzdKK90JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Dispositivo utilizado: {device}\")"
      ],
      "metadata": {
        "id": "kQQJhh_S8Hhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "306d8d7c-f1b8-4e35-e133-9891ab14dfc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo utilizado: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de Datos\n",
        "*    Reutilizamos y adaptamos el código de preprocesamiento de la Semana 2\n",
        "*    Configuración de rutas y parámetros"
      ],
      "metadata": {
        "id": "YaKh-ikg96N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/descomprimido/Garbage classification/Garbage classification/'\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 224\n",
        "NUM_EPOCHS = 15\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "8qDuNM1QN5z9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clases del dataset"
      ],
      "metadata": {
        "id": "fvWBuVMgN7sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "print(f\"Clases: {CLASSES}\")\n",
        "print(f\"Número de clases: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywq6C8zQN7Ap",
        "outputId": "c5ae5bab-c9f2-4e93-fd9c-a939eceb726b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
            "Número de clases: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WasteDataset(Dataset):\n",
        "    \"\"\"Dataset personalizado para clasificación de residuos\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.classes = CLASSES\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        # Cargar rutas de imágenes y etiquetas\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_path = os.path.join(data_path, class_name)\n",
        "            if os.path.exists(class_path):\n",
        "                for img_name in os.listdir(class_path):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        self.images.append(os.path.join(class_path, img_name))\n",
        "                        self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "q7kjAED6N-jp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformaciones para entrenamiento (con data augmentation)"
      ],
      "metadata": {
        "id": "r6TbNiuDOBE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "WdKYI-rPODsm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicación\n",
        "* transforms.Resize((IMG_SIZE, IMG_SIZE)):\n",
        "    > Función: Redimensiona todas las imágenes a 224x224 píxeles  \n",
        "    > Por qué: Las imágenes vienen en diferentes tamaños, pero la red necesita entrada uniforme  \n",
        "    > Ejemplo: Una imagen de 1920x1080 se convierte a 224x224\n",
        "\n",
        "* transforms.RandomHorizontalFlip(p=0.5):\n",
        "\n",
        "    > Función: Voltea horizontalmente la imagen con 50% de probabilidad  \n",
        "    > Por qué: Data Augmentation - crea variaciones para evitar overfitting  \n",
        "    > Ejemplo: Una botella de plástico puede aparecer normal o volteada horizontalmente\n",
        "\n",
        "* transforms.RandomRotation(degrees=15)\n",
        "\n",
        "    > Función: Rota la imagen aleatoriamente entre -15° y +15°  \n",
        "    > Por qué: Los residuos en fotos reales pueden estar en cualquier ángulo  \n",
        "    > Ejemplo: Una lata puede estar inclinada 10° a la izquierda o derecha\n",
        "\n",
        "* transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "\n",
        "    > Función: Modifica aleatoriamente:\n",
        "        - brightness=0.2: Brillo ±20%\n",
        "        - contrast=0.2: Contraste ±20%\n",
        "        - saturation=0.2: Saturación ±20%\n",
        "        - hue=0.1: Matiz ±10%  \n",
        "    > Por qué: Simula diferentes condiciones de iluminación y cámara  \n",
        "    > Ejemplo: Una foto puede ser más brillante, más oscura, o con colores más intensos\n",
        "\n",
        "* transforms.ToTensor()\n",
        "\n",
        "    > Función: Convierte la imagen PIL a tensor de PyTorch  \n",
        "    > Cambios:\n",
        "        - Formato: (H, W, C) → (C, H, W)\n",
        "        - Valores: [0-255] → [0.0-1.0]  \n",
        "    > Por qué: PyTorch necesita tensores, no imágenes PIL\n",
        "\n",
        "* transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    > Función: Normaliza cada canal RGB con media y desviación estándar específicas  \n",
        "    > Valores: Los de ImageNet (dataset donde se preentrenaron MobileNet y ResNet)\n",
        "      - Media: [0.485, 0.456, 0.406] (R, G, B)\n",
        "      - Std: [0.229, 0.224, 0.225] (R, G, B)  \n",
        "    > Fórmula: pixel_normalizado = (pixel - media) / std  \n",
        "    > Por qué: Los modelos preentrenados esperan datos en este rango\n",
        "\n",
        "* Objetivo General:\n",
        "    Estas transformaciones crean múltiples versiones de cada imagen durante el entrenamiento:\n",
        "    Imagen Original → [Resize] → [Flip?] → [Rotate] → [ColorJitter] → [ToTensor] → [Normalize]\n",
        "    Beneficios:\n",
        "\n",
        "    Previene overfitting: El modelo ve más variaciones\n",
        "    Mejora generalización: Reconoce residuos en diferentes condiciones\n",
        "    Aumenta dataset virtual: Una imagen se convierte en miles de variaciones\n",
        "    Robustez: Funciona con fotos tomadas en diferentes ángulos/iluminación"
      ],
      "metadata": {
        "id": "y-PuoYCpOHwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformaciones para validación (sin augmentation)"
      ],
      "metadata": {
        "id": "H1wQwwBdOLQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "5oA9w70qOMqE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la validación se evalúa el modelo tal como esta, no buscando variaciones"
      ],
      "metadata": {
        "id": "ut93cOu4OOeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear dataset completo y dividir en train/val"
      ],
      "metadata": {
        "id": "hXu9QlsjOVCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = WasteDataset(DATA_PATH, transform=train_transform)\n",
        "dataset_size = len(full_dataset)\n",
        "print(f\"Tamaño total del dataset: {dataset_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx6OPFI5OROn",
        "outputId": "ec551aec-2687-4748-8e88-1b42794bacfc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño total del dataset: 2527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# División 80/20 para entrenamiento y validación"
      ],
      "metadata": {
        "id": "MeCc2TBwOYh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")"
      ],
      "metadata": {
        "id": "Ia1L-BXoOZau"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* se utilizan 80% de los datos para entrenamiento y 20% de los datos para validación siendo el total del dataset de 2527 muestras es un buen porcentaje para seleccionar\n",
        "* se utiliza manual_seed(42) para siempre obtener la misma generación de datos aleatorios, esto es así para pode controlar si existe un error y el número 42 es un número común utilizado"
      ],
      "metadata": {
        "id": "uiYbSf-2OcHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicar transformaciones específicas a validación"
      ],
      "metadata": {
        "id": "gkZGbjUhOel1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset.dataset.transform = val_transform\n",
        "\n",
        "print(f\"Tamaño dataset entrenamiento: {len(train_dataset)}\")\n",
        "print(f\"Tamaño dataset validación: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXhjQtWHOfvL",
        "outputId": "e11f3337-b2b1-4e86-f93f-a2db8c235d6c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño dataset entrenamiento: 2021\n",
            "Tamaño dataset validación: 506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear DataLoaders"
      ],
      "metadata": {
        "id": "lXIRZbenOhBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "7Ah2VSI-OifY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener distribución del dataset de entrenamiento"
      ],
      "metadata": {
        "id": "Lz2PyQZhOsfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
        "class_counts = Counter(train_labels)\n",
        "\n",
        "print(\"Distribución de clases en entrenamiento:\")\n",
        "for class_idx, count in sorted(class_counts.items()):\n",
        "    print(f\"{CLASSES[class_idx]}: {count} imágenes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDrtO6SZOte_",
        "outputId": "471fb1d2-7b3f-45aa-9e83-897bf2d1a173"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribución de clases en entrenamiento:\n",
            "cardboard: 326 imágenes\n",
            "glass: 396 imágenes\n",
            "metal: 324 imágenes\n",
            "paper: 478 imágenes\n",
            "plastic: 393 imágenes\n",
            "trash: 104 imágenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calcular pesos para el loss balanceado"
      ],
      "metadata": {
        "id": "xA5HToaMOvga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = sum(class_counts.values())\n",
        "class_weights = []\n",
        "for i in range(NUM_CLASSES):\n",
        "    weight = total_samples / (NUM_CLASSES * class_counts.get(i, 1))\n",
        "    class_weights.append(weight)\n",
        "\n",
        "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
        "print(f\"\\nPesos de clases para loss balanceado: {class_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMEbmJK3OwgW",
        "outputId": "982f993f-b630-4060-c29a-6b9c28a29d67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pesos de clases para loss balanceado: [1.0332310838445808, 0.8505892255892256, 1.0396090534979423, 0.7046722454672245, 0.8570822731128075, 3.238782051282051]\n"
          ]
        }
      ]
    }
  ]
}